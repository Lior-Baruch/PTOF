{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# %pip install -q -U transformers trl peft bitsandbytes openai numpy==1.26.4 pandas==2.2.2 torch==2.4.0\n",
    "# %pip install -q -U transformers==4.44.0 trl==0.9.6 peft==0.12.0 bitsandbytes openai numpy==1.26.4 pandas==2.2.2 torch==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qqq flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.2\n",
      "torch version: 2.4.0+cu118\n",
      "Using device: cuda\n",
      "cuda version: 11.8\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from system_prompts_builder import generate_all_permutations, CounselorPersonality\n",
    "from questionnaires import get_prompt_eval_questionnaire, get_prompt_eval_questionnaire_partial_conv\n",
    "import textwrap\n",
    "import json\n",
    "import os\n",
    "import tqdm \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import setup_chat_format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# print versions of the above libraries\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "# import peft\n",
    "# import trl \n",
    "# import transformers\n",
    "# print(\"peft version:\", peft.__version__)\n",
    "# print(\"trl version:\", trl.__version__)\n",
    "# print(\"transformers version:\", transformers.__version__)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"cuda version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "    print(\"Using flash_attention_2\")\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "    print(\"Using eager\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OpenAI client to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# get key from file\n",
    "\n",
    "# OpenAI_API_KEY = \"Put your key here\"\n",
    "OpenAI_API_KEY = open(\"openai_key.txt\", \"r\").read().strip()\n",
    "\n",
    "# Create a client instance with your API key\n",
    "client = OpenAI(\n",
    "    api_key=OpenAI_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the therapist model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# log in to the Hugging Face hub (required for private datasets/models)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load the 7b llama-2 model\n",
    "therapist_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(therapist_model_id, trust_remote_code=True, device_map=device)\n",
    "\n",
    "# tokenizer.add_tokens(['<|im_start|>', '<|im_end|>'], special_tokens=False)\n",
    "# tokenizer.bos_token = '<|im_start|>'\n",
    "# tokenizer.eos_token = '<|im_end|>'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # # Chat format\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# prints, can delete later\n",
    "#print(f\"tokenizer.chat_template: {tokenizer.chat_template}\")\n",
    "print(\"Spacial tokens: \", tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_massage = [{'role': 'system', 'content': 'you are a robot'}, \n",
    "                {'role': 'user', 'content': 'Hello, how is it to be a robot?'},\n",
    "                {'role': 'assistant', 'content': 'It is great to be a robot.'},\n",
    "                {'role': 'user', 'content': 'cool.'}]\n",
    "\n",
    "\n",
    "chat = tokenizer.apply_chat_template(test_massage, tokenize=False, add_generation_prompt=True)\n",
    "encoded_chat = tokenizer.encode(chat, return_tensors=\"pt\", add_special_tokens=False)\n",
    "tokens_chat = tokenizer.convert_ids_to_tokens(encoded_chat[0])\n",
    "decoded_chat = tokenizer.decode(encoded_chat[0], skip_special_tokens=True)\n",
    "# print(f\"test_massage: {test_massage}\")\n",
    "print(f\"chat: {chat}\\n\")\n",
    "# print(f\"tokens_chat: {tokens_chat}\\n\")\n",
    "# print(f\"dencoded_chat: {decoded_chat}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Set quantization config (to save memory)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "############################################\n",
    "# Load model, quantized\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    therapist_model_id, \n",
    "    quantization_config=quantization_config, \n",
    "    device_map=device, \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "# base_model.resize_token_embeddings(len(tokenizer)) # Resize model embeddings to include new tokens\n",
    "\n",
    "\n",
    "\n",
    "# #match tokenizer spacial tokens to model\n",
    "# base_model.config.bos_token_id = tokenizer.bos_token_id\n",
    "# base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "# base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "# base_model.config.decoder_end_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose and Add Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookAhead = 5\n",
    "\n",
    "########################################### New Adapters ############################################\n",
    "########################################### New Adapters ############################################\n",
    "therapist_first_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V1\"\n",
    "therapist_second_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V2\"\n",
    "\n",
    "\n",
    "\n",
    "########################################### OLD Adapters ############################################\n",
    "########################################### OLD Adapters ############################################\n",
    "### Look-Ahead=0 PartialEval ###\n",
    "# therapist_first_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-PartialEval_ET0.1_MT1.2_1-5_V.1.0_Filtered0.1_V1.0\"\n",
    "# therapist_second_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-PartialEval_ET0.1_MT1.2_1-5_V.1.0_Filtered0.1_V2.0\"\n",
    "# therapist_third_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-PartialEval_ET0.1_MT1.2_1-5_V.1.0_Filtered0.1_V3.0\"\n",
    "\n",
    "### Look-Ahead=3 FullEval ###\n",
    "# therapist_first_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-LookAhead3_FullEval_TTree1.4_TLoop0.7_TEval0.2_Filter0.2_V1.0\"\n",
    "# therapist_second_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-LookAhead3_FullEval_TTree1.4_TLoop0.7_TEval0.2_Filter0.2_V2.0\"\n",
    "# therapist_third_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-LookAhead3_FullEval_TTree1.4_TLoop0.7_TEval0.2_Filter0.2_V3.0\"\n",
    "# therapist_fourth_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-LookAhead3_FullEval_TTree1.4_TLoop0.7_TEval0.2_Filter0.2_V4.0\"\n",
    "\n",
    "\n",
    "### Look-Ahead=5 PartialEval ###\n",
    "# therapist_first_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-PartialEval_LookAhead5_ET0.1_MT1.2_1-5_Filtered0.1_V1.0\"\n",
    "# therapist_second_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-PartialEval_LookAhead5_ET0.1_MT1.2_1-5_Filtered0.1_V2.0\"\n",
    "\n",
    "### Look-Ahead=5 FullEval ###\n",
    "# therapist_first_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-FullEval_LookAhead5_TTree1.2_TT0.7_TP0.7_TE0.1_Filtered0.1_V1.0\"\n",
    "# therapist_second_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-FullEval_LookAhead5_TTree1.2_TT0.7_TP0.7_TE0.1_Filtered0.1_V2.0\"\n",
    "# therapist_third_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-FullEval_LookAhead5_TTree1.2_TT0.7_TP0.7_TE0.1_Filtered0.1_V3.0\"\n",
    "# therapist_fourth_adapter_id = \"LBK95/Llama-2-7b-hf-DPO-FullEval_LookAhead5_TTree1.2_TT0.7_TP0.7_TE0.1_Filtered0.1_V4.0\"\n",
    "########################################### OLD Adapters ############################################\n",
    "########################################### OLD Adapters ############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################### Add Adapters ############################################\n",
    "# add first adapter if therapist_first_adapter_id is defined\n",
    "base_model = PeftModel.from_pretrained(base_model, therapist_first_adapter_id)\n",
    "# merge first adapter and unload\n",
    "base_model = base_model.merge_and_unload()\n",
    "print(\"Model loaded with first adapter\")\n",
    "print(\"Adapter ID: \", therapist_first_adapter_id)\n",
    "\n",
    "# add second adapter\n",
    "base_model = PeftModel.from_pretrained(base_model, therapist_second_adapter_id)\n",
    "# merge second adapter and unload\n",
    "base_model = base_model.merge_and_unload()\n",
    "print(\"Model loaded with second adapter\")\n",
    "print(\"Adapter ID: \", therapist_second_adapter_id)\n",
    "\n",
    "# # add third adapter\n",
    "# base_model = PeftModel.from_pretrained(base_model, therapist_third_adapter_id)\n",
    "# # merge third adapter and unload\n",
    "# base_model = base_model.merge_and_unload()\n",
    "# print(\"Model loaded with third adapter\")\n",
    "# print(\"Adapter ID: \", therapist_third_adapter_id)\n",
    "\n",
    "# # add fourth adapther\n",
    "# base_model = PeftModel.from_pretrained(base_model, therapist_fourth_adapter_id)\n",
    "# # merge fourth adapter and unload\n",
    "# base_model = base_model.merge_and_unload()\n",
    "# print(\"Model loaded with fourth adapter\")\n",
    "# print(\"Adapter ID: \", therapist_fourth_adapter_id)\n",
    "# ########################################### Add Adapters ############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "- concat_conversation\n",
    "- print_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def concat_conversation(conversation):\n",
    "    \"\"\"\n",
    "    Concatenate the conversation into a single string.\n",
    "\n",
    "    Parameters:\n",
    "        - conversation: A list of strings representing the conversation. (Therapist and Patient messages alternately, starting with the Therapist)\n",
    "\n",
    "    Returns:\n",
    "        - conversation_str: A single string representing the conversation.\n",
    "    \"\"\"\n",
    "    conversation_str = \"\"\n",
    "    for i, message in enumerate(conversation):\n",
    "        role = \"[THERAPIST]\" if i % 2 == 0 else \"[PATIENT]\"\n",
    "        conversation_str += f\"{role}: \\n{message} \\n\\n\"\n",
    "    return conversation_str\n",
    "\n",
    "def print_conversation(conversation, max_width=80):\n",
    "    \"\"\"\n",
    "    Print the conversation with word wrapping.\n",
    "\n",
    "    Parameters:\n",
    "        - conversation: A list of strings representing the conversation. (Therapist and Patient messages alternately, starting with the Therapist)\n",
    "        - max_width: The maximum width for word wrapping. Default is 80.\n",
    "    \"\"\"\n",
    "    for i, message in enumerate(conversation):\n",
    "        role = \"[THERAPIST]\" if i % 2 == 0 else \"[PATIENT]\"\n",
    "        print(f\"{role}: \\n{textwrap.fill(message, width=max_width)} \\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a conversation between a therapist and a patient.\n",
    "- synthesize_conversation_therapistModel_patientOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patient_response(client, model_id, messages_Patient_assist, max_tokens, temperature):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages_Patient_assist,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        seed=42\n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    return response_content\n",
    "\n",
    "def generate_therapist_responses(therapist_model, therapis_tokenizer, messages_Therapist_assist, max_tokens, temperature, num_responses=2, max_attempts=10):\n",
    "    valid_responses = []\n",
    "    \n",
    "    while len(valid_responses) < num_responses and max_attempts > 0:\n",
    "        max_attempts -= 1\n",
    "        # Apply the chat template and encode the prompt\n",
    "        prompt = therapis_tokenizer.apply_chat_template(messages_Therapist_assist, tokenize=False, add_generation_prompt=True)\n",
    "        input_ids = therapis_tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(therapist_model.device)\n",
    "        \n",
    "        # Generate responses for the therapist\n",
    "        responses = therapist_model.generate(\n",
    "            input_ids, \n",
    "            do_sample=True, \n",
    "            max_new_tokens=max_tokens, \n",
    "            pad_token_id=therapis_tokenizer.eos_token_id, \n",
    "            eos_token_id=therapis_tokenizer.eos_token_id, \n",
    "            temperature=temperature, \n",
    "            num_return_sequences=num_responses - len(valid_responses),  # Adjust the number of responses needed\n",
    "            stop_strings=[\"<|im_end|>\"],  # Stop generation at \"<|im_end|>\"\n",
    "            tokenizer=therapis_tokenizer\n",
    "        )\n",
    "\n",
    "        # Decode and filter responses\n",
    "        for response in responses:\n",
    "            decoded_response_original = therapis_tokenizer.decode(response[len(input_ids[0]):], skip_special_tokens=True)\n",
    "            decoded_response = decoded_response_original.split(\"<|im_end|>\")[0]  # Get only up until \"<|im_end|>\"\n",
    "            decoded_response = decoded_response.split(\"<|im_start|>\")[0] # Get only up until \"<|im_start|>\"\n",
    "            decoded_response = decoded_response.split(\"<|\")[0] # Get only up until \"<|\"\n",
    "            decoded_response = decoded_response.split(\"|>\")[0] # Get only up until \"|>\"\n",
    "            # print length of original and filtered response\n",
    "            print(f\"Original, Filtered: {len(decoded_response_original)}, {len(decoded_response)}\")\n",
    "            # Filter out empty or invalid responses\n",
    "            if decoded_response.strip():  # Check if the response is not empty after stripping whitespaces\n",
    "                valid_responses.append(decoded_response)\n",
    "            else:\n",
    "                print(\"Invalid original response: \", decoded_response_original)\n",
    "\n",
    "    if len(valid_responses) < num_responses:\n",
    "        print(\"Could not generate the required number of valid responses.\")\n",
    "        return None\n",
    "\n",
    "    return valid_responses # Return the required number of valid responses\n",
    "\n",
    "\n",
    "def handle_session_end(response_content, turn_index):\n",
    "    session_ended_keyword = \"SESSION ENDED\"\n",
    "    idx = response_content.upper().find(session_ended_keyword)\n",
    "    \n",
    "    if idx != -1:\n",
    "        session_endded_explanation = response_content[idx + len(session_ended_keyword):]\n",
    "        response_content = response_content[:idx]\n",
    "        session_endded_by = \"patient\" if turn_index % 2 == 0 else \"therapist\"\n",
    "        print(\"Response content (SESSION ENDED): \", response_content)\n",
    "        print(\"Session ended by: \", session_endded_by)\n",
    "        return session_endded_by, session_endded_explanation, response_content\n",
    "    else:\n",
    "        raise ValueError(\"SESSION ENDED keyword not found in response content\")\n",
    "\n",
    "\n",
    "def update_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, role_Patient, role_Therapist, response_content):\n",
    "    conversation.append(response_content)\n",
    "    messages_Patient_assist.append({\"role\": role_Patient, \"content\": response_content})\n",
    "    messages_Therapist_assist.append({\"role\": role_Therapist, \"content\": response_content})\n",
    "\n",
    "########################################################################################\n",
    "def initialize_conversation(system_prompt_therapist, system_prompt_patient, therapist_init_utterance, patient_init_utterance):\n",
    "    # Initialize the conversation\n",
    "    conversation = [therapist_init_utterance]  # Therapist's initial utterance\n",
    "\n",
    "    # Initialize the messages for the patient as the assistant\n",
    "    messages_Patient_assist = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_patient},\n",
    "        {\"role\": \"user\", \"content\": therapist_init_utterance}  # Therapist's initial utterance\n",
    "    ]\n",
    "\n",
    "    # Initialize the messages for the therapist as the assistant\n",
    "    messages_Therapist_assist = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_therapist},\n",
    "        {\"role\": \"user\", \"content\": patient_init_utterance},  # Patient's initial utterance (llama model needs a user utterance to start)\n",
    "        {\"role\": \"assistant\", \"content\": therapist_init_utterance}  # Therapist's initial utterance\n",
    "    ]\n",
    "\n",
    "    return conversation, messages_Patient_assist, messages_Therapist_assist\n",
    "\n",
    "\n",
    "def conversation_loop(conversation, messages_Patient_assist, messages_Therapist_assist, therapist_model, therapist_tokenizer, client,\n",
    "                      model_id=\"gpt-3.5-turbo\", max_tokens=100, num_utterances=6, temperature_therapist=0.7, temperature_patient=0.7):\n",
    "    session_endded_by = None\n",
    "    session_endded_explanation = None\n",
    "\n",
    "    for i in range(num_utterances):\n",
    "        if i % 2 == 0:  # Patient's turn\n",
    "            role_Patient = \"assistant\"\n",
    "            role_Therapist = \"user\"\n",
    "            response_content = generate_patient_response(client, model_id, messages_Patient_assist, max_tokens, temperature_patient)\n",
    "            print(\"[Patient]: \", response_content)\n",
    "        else:  # Therapist's turn\n",
    "            role_Patient = \"user\"\n",
    "            role_Therapist = \"assistant\"\n",
    "            responses = generate_therapist_responses(therapist_model, therapist_tokenizer, messages_Therapist_assist, max_tokens, temperature_therapist, 1)\n",
    "            \n",
    "            if responses is None:\n",
    "                print(\"Could not generate a valid response for the therapist, returning None.\")\n",
    "                return None, None, None, None, None\n",
    "            \n",
    "            response_content = responses[0] # Extract the response from the list\n",
    "            print(\"-\" * 50)\n",
    "            print(\"[Therapist]: \", response_content)\n",
    "\n",
    "        # Check if the session has ended\n",
    "        if \"SESSION ENDED\" in response_content.upper():\n",
    "            session_endded_by, session_endded_explanation, response_content = handle_session_end(response_content, i)\n",
    "            if response_content: # Add the last message before session ended\n",
    "                update_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, role_Patient, role_Therapist, response_content)\n",
    "            break\n",
    "        # Update the conversation\n",
    "        update_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, role_Patient, role_Therapist, response_content)\n",
    "\n",
    "    return conversation, messages_Patient_assist, messages_Therapist_assist, session_endded_by, session_endded_explanation\n",
    "\n",
    "\n",
    "def synthesize_conversation_therapistModel_patientOpenAI(system_prompt_therapist, system_prompt_patient, therapist_init_utterance, \n",
    "                                                         patient_init_utterance, therapist_model, therapis_tokenizer, client, model_id=\"gpt-3.5-turbo\",  \n",
    "                                                         max_tokens=50, num_utterances=6, temperature_therapist=0.7, temperature_patient=0.7):\n",
    "    # Initialize the conversation\n",
    "    conversation, messages_Patient_assist, messages_Therapist_assist = initialize_conversation(system_prompt_therapist, system_prompt_patient, therapist_init_utterance, patient_init_utterance)\n",
    "    print(\"Therapist's initial utterance: \", therapist_init_utterance)\n",
    "    # Start the conversation loop\n",
    "    return conversation_loop(conversation, messages_Patient_assist, messages_Therapist_assist, therapist_model, therapis_tokenizer, client, model_id, max_tokens, num_utterances, temperature_therapist, temperature_patient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a conversation\n",
    "- eval_conversation\n",
    "- extract_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def eval_conversation(conversation_str, client, questionnaire=13, model_id=\"gpt-3.5-turbo\", temperature=0.2, max_token_per_question=100, partial_conv=False, look_ahead=3):\n",
    "    # Generate the evaluation prompt\n",
    "    if partial_conv:\n",
    "        eval_dict = get_prompt_eval_questionnaire_partial_conv(\n",
    "            questionnaire=questionnaire,\n",
    "            conversation=conversation_str,\n",
    "            look_ahead=look_ahead\n",
    "        )\n",
    "    else:\n",
    "        eval_dict = get_prompt_eval_questionnaire(\n",
    "            questionnaire=questionnaire,\n",
    "            conversation=conversation_str\n",
    "        )\n",
    "\n",
    "    eval_prompt = eval_dict[\"prompt\"]\n",
    "    eval_questions_count = eval_dict[\"questions_count\"]\n",
    "\n",
    "    # Generate the completion for evaluation\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "        #messages=[{\"role\": \"system\", \"content\": eval_prompt}],\n",
    "        max_tokens=max_token_per_question * eval_questions_count,\n",
    "        temperature=temperature,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Extract the evaluation results\n",
    "    eval_results = response.choices[0].message.content\n",
    "    # print(\"eval_results: \", eval_results)\n",
    "\n",
    "    return eval_results, eval_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(evaluation_text):\n",
    "    # Regular expression pattern to match numbers\n",
    "    pattern = re.compile(r'\\b\\d+\\b')\n",
    "\n",
    "    # Find all occurrences of the pattern\n",
    "    scores = pattern.findall(evaluation_text)\n",
    "    # Convert found strings to integers\n",
    "    scores = [int(score) for score in scores if score.isdigit()] # includes the question number\n",
    "    # get only odd indexes (the scores without the question number)\n",
    "    scores = scores[1::2]\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to synthesize conversation trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lookahead_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, response):\n",
    "    conversation_new = conversation + [response]\n",
    "    messages_Patient_assist_new = messages_Patient_assist.copy() + [{\"role\": \"user\", \"content\": response}]\n",
    "    messages_Therapist_assist_new = messages_Therapist_assist.copy() + [{\"role\": \"assistant\", \"content\": response}]\n",
    "    return conversation_new, messages_Patient_assist_new, messages_Therapist_assist_new\n",
    "\n",
    "\n",
    "def eval_conversation_with_questionnaires(conversation_str, client, questionnaire_list, model_id, temperature_eval, max_token_per_question, partial_conv, look_ahead, questionnaire_scalars=[0.5, 0.5], max_attempts=3):\n",
    "    scores_list = []\n",
    "    scores_avg_list = []\n",
    "    results_list = []\n",
    "    for questionnaire in questionnaire_list:\n",
    "        flag = False\n",
    "        while not flag and max_attempts > 0:\n",
    "            max_attempts -= 1\n",
    "            eval_results, eval_dict = eval_conversation(\n",
    "                conversation_str, \n",
    "                client, \n",
    "                questionnaire=questionnaire, \n",
    "                model_id=model_id, \n",
    "                temperature=temperature_eval, \n",
    "                max_token_per_question=max_token_per_question, \n",
    "                partial_conv=partial_conv,\n",
    "                look_ahead=look_ahead\n",
    "            )\n",
    "            questions_count = eval_dict[\"questions_count\"]\n",
    "            scores = extract_scores(eval_results)\n",
    "            if len(scores) == questions_count:\n",
    "                flag = True\n",
    "                results_list.append(eval_results)\n",
    "                scores_list.append(scores)\n",
    "                scores_avg_list.append(np.mean(scores))\n",
    "            else:\n",
    "                print(\"questions_count, scores: \", questions_count, len(scores))\n",
    "                print(\"Scores count does not match the questions count. Retrying...\")\n",
    "\n",
    "        if not flag:\n",
    "            print(\"Could not generate the required number of valid scores for the questionnaire: \", questionnaire)\n",
    "            return None, None, None, None\n",
    "        \n",
    "    final_score = np.dot(scores_avg_list, questionnaire_scalars)\n",
    "    \n",
    "    return final_score, scores_avg_list, scores_list, results_list\n",
    "\n",
    "\n",
    "def determine_winner(responses, scores_one_list, scores_two_list, scores_one_avg_list, scores_two_avg_list, \n",
    "                     score_one_final, score_two_final, conversation_one, conversation_two):\n",
    "    \n",
    "    if score_one_final >= score_two_final: # Response one is the winner\n",
    "        return (responses[0], responses[1], scores_one_list, scores_two_list, scores_one_avg_list, scores_two_avg_list, score_one_final, score_two_final, conversation_one, conversation_two)\n",
    "    else: # Response two is the winner\n",
    "        return (responses[1], responses[0], scores_two_list, scores_one_list, scores_two_avg_list, scores_one_avg_list, score_two_final, score_one_final, conversation_two, conversation_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_conversation_tree_lookAhead(init_messages_Patient_assist, init_messages_Therapist_assist, init_conversation, \n",
    "                                                     therapist_model, therapis_tokenizer, client, model_id=\"gpt-3.5-turbo\", max_tokens=100, \n",
    "                                                     num_utterances=6, temperature_tree=1.2,temperature_therapist=0.7, temperature_patient=0.7, temperature_eval=0.2,\n",
    "                                                     questionnaire_list=[13, 14], questionnaire_scalars=[1, 1], max_token_per_question=100, partial_conv=False, look_ahead=3):\n",
    "    \n",
    "    if init_messages_Patient_assist is None:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    preference_data = []\n",
    "    messages_Patient_assist = init_messages_Patient_assist.copy()\n",
    "    messages_Therapist_assist = init_messages_Therapist_assist.copy()\n",
    "    conversation = init_conversation.copy()\n",
    "    session_endded_by = None\n",
    "    session_endded_explanation = None\n",
    "\n",
    "    for i in (range(num_utterances)):\n",
    "        if i % 2 == 1:  # Patient's turn\n",
    "            role_Patient = \"assistant\"\n",
    "            role_Therapist = \"user\"\n",
    "            response_content = generate_patient_response(client, model_id, messages_Patient_assist, max_tokens, temperature_patient)\n",
    "            print(\"\\n Patient response: \\n\", response_content)\n",
    "        else:  # Therapist's turn\n",
    "            print(\"-\" * 80)\n",
    "            print(\"Tree utterance number: \", i)\n",
    "            role_Patient = \"user\"\n",
    "            role_Therapist = \"assistant\"\n",
    "            # generate two responses for the therapist (branching)\n",
    "            responses = generate_therapist_responses(therapist_model, therapis_tokenizer, messages_Therapist_assist, max_tokens, temperature_tree, 2)\n",
    "            if responses is None:\n",
    "                print(\"Could not generate the required number of valid responses. Skipping...\")\n",
    "                return None, None, None, None, None, None\n",
    "            \n",
    "\n",
    "            # handle SESSION ENDED\n",
    "            for response in responses:\n",
    "                if \"SESSION ENDED\" in response.upper():\n",
    "                    print(\"Response with SESSION ENDED: \", response)\n",
    "                    session_endded_by, session_endded_explanation, response = handle_session_end(response, i) # handle session ended\n",
    "                    print(\"Response after handling SESSION ENDED: \", response)\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            print(\"Therapist responses: \\n\", responses)\n",
    "\n",
    "            # split the conversation to two branches and continue each branch for look_ahead steps\n",
    "            conversation_one, messages_Patient_assist_one, messages_Therapist_assist_one = prepare_lookahead_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, responses[0])\n",
    "            conversation_two, messages_Patient_assist_two, messages_Therapist_assist_two = prepare_lookahead_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, responses[1])\n",
    "\n",
    "            # continue the conversation for look_ahead steps\n",
    "            print(\"-\" * 40 + \"Response one Look-Ahead\" + \"-\" * 40)\n",
    "            print(\"response one: \", responses[0])\n",
    "            \n",
    "            conversation_one, messages_Patient_assist_one, messages_Therapist_assist_one, _, _ = conversation_loop(\n",
    "                conversation_one, messages_Patient_assist_one, messages_Therapist_assist_one, therapist_model, therapis_tokenizer, client, model_id, max_tokens, look_ahead, temperature_therapist, temperature_patient)\n",
    "            print(\"-\" * 40 + \"Response two Look-Ahead\" + \"-\" * 40)\n",
    "            print(\"response two: \", responses[1])\n",
    "            conversation_two, messages_Patient_assist_two, messages_Therapist_assist_two, _, _ = conversation_loop(\n",
    "                conversation_two, messages_Patient_assist_two, messages_Therapist_assist_two, therapist_model, therapis_tokenizer, client, model_id, max_tokens, look_ahead, temperature_therapist, temperature_patient)\n",
    "            \n",
    "            if conversation_one is None or conversation_two is None:\n",
    "                print(\"Could not generate the required number of valid responses for the look-ahead steps. Skipping...\")\n",
    "                return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "            # evaluate the two conversations\n",
    "            score_one_final, scores_one_avg_list, scores_one_list, _ = eval_conversation_with_questionnaires(concat_conversation(conversation_one), client, questionnaire_list, model_id, temperature_eval, max_token_per_question, partial_conv, look_ahead, questionnaire_scalars)\n",
    "            score_two_final, scores_two_avg_list, scores_two_list, _ = eval_conversation_with_questionnaires(concat_conversation(conversation_two), client, questionnaire_list, model_id, temperature_eval, max_token_per_question, partial_conv, look_ahead, questionnaire_scalars)\n",
    "            \n",
    "            if score_one_final is None or score_two_final is None:\n",
    "                print(\"Could not generate the required number of valid scores for the conversations. Skipping...\")\n",
    "                return None, None, None, None, None, None\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            print(\"score_one_final: \", score_one_final)\n",
    "            print(\"score_two_final: \", score_two_final)\n",
    "\n",
    "            # determine the winner and loser\n",
    "            winning_response, losing_response, winning_scores_list, losing_scores_list, winning_scores_avg_list, losing_scores_avg_list, winning_score_final, losing_score_final, winning_conversation, losing_conversation = determine_winner(\n",
    "                responses, scores_one_list, scores_two_list, scores_one_avg_list, scores_two_avg_list, score_one_final, score_two_final, conversation_one, conversation_two)\n",
    "            print(\"-\" * 80)\n",
    "            print(\"winning_response: \", winning_response)\n",
    "            print(\"losing_response: \", losing_response)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            # Append the preference data\n",
    "            preference_data.append({\n",
    "                \"conversation\": conversation.copy(),\n",
    "                \"messages\": messages_Therapist_assist.copy(),\n",
    "                \"winning_response\": winning_response,\n",
    "                \"losing_response\": losing_response,\n",
    "                \"winning_scores_list\": winning_scores_list,\n",
    "                \"losing_scores_list\": losing_scores_list,\n",
    "                \"winning_scores_avg_list\": winning_scores_avg_list, \n",
    "                \"losing_scores_avg_list\": losing_scores_avg_list,\n",
    "                \"winning_score_final\": winning_score_final,\n",
    "                \"losing_score_final\": losing_score_final,\n",
    "                \"winning_conversation\": winning_conversation,\n",
    "                \"losing_conversation\": losing_conversation,\n",
    "            })\n",
    "\n",
    "            response_content = winning_response\n",
    "\n",
    "        if \"SESSION ENDED\" in response_content.upper():\n",
    "            session_endded_by, session_endded_explanation, response_content = handle_session_end(response_content, i)\n",
    "            if response_content:\n",
    "                update_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, role_Patient, role_Therapist, response_content)\n",
    "            break # End the conversation\n",
    "\n",
    "        update_conversation(conversation, messages_Patient_assist, messages_Therapist_assist, role_Patient, role_Therapist, response_content)\n",
    "\n",
    "    return preference_data, messages_Patient_assist, messages_Therapist_assist, conversation, session_endded_by, session_endded_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def synthesize_conversation_tree_lookAhead_for_permutation(system_prompt_therapist, system_prompt_patient, therapist_init_utterance, patient_init_utterance, therapist_model, therapis_tokenizer, client, model_id=\"gpt-3.5-turbo\",\n",
    "                                 max_tokens=50, num_init_utterances=2, num_tree_utterances=6, temperature_tree=1.2, temperature_conversation_Therapist = 0.7, temperature_conversation_Patient = 0.7, temperature_eval=0.1, \n",
    "                                 questionnaire_list=[13, 14], questionnaire_scalars=[0.5, 0.5], max_token_per_question=100, partial_conv=False, look_ahead=3):\n",
    "\n",
    "    # Create the initial conversation (no tree)\n",
    "    init_conversation, init_messages_Patient_assist, init_messages_Therapist_assist, session_endded_by, session_endded_explanation = synthesize_conversation_therapistModel_patientOpenAI(\n",
    "        system_prompt_therapist=system_prompt_therapist,\n",
    "        system_prompt_patient=system_prompt_patient,\n",
    "        therapist_init_utterance=therapist_init_utterance,\n",
    "        patient_init_utterance=patient_init_utterance,\n",
    "        therapist_model=therapist_model,\n",
    "        therapis_tokenizer=therapis_tokenizer,\n",
    "        client=client,\n",
    "        model_id=model_id,\n",
    "        max_tokens=max_tokens,\n",
    "        num_utterances=num_init_utterances,\n",
    "        temperature_therapist=temperature_conversation_Therapist,\n",
    "        temperature_patient=temperature_conversation_Patient\n",
    "    )\n",
    "\n",
    "    if init_conversation is None:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    # Create the conversation tree (with look-ahead evaluation)\n",
    "    preference_data, messages_Patient_assist, messages_Therapist_assist, conversation, session_endded_by, session_endded_explanation = synthesize_conversation_tree_lookAhead(\n",
    "        init_messages_Patient_assist=init_messages_Patient_assist,\n",
    "        init_messages_Therapist_assist=init_messages_Therapist_assist,\n",
    "        init_conversation=init_conversation,\n",
    "        therapist_model=therapist_model,\n",
    "        therapis_tokenizer=therapis_tokenizer,\n",
    "        client=client,\n",
    "        model_id=model_id,\n",
    "        max_tokens=max_tokens,\n",
    "        num_utterances=num_tree_utterances,\n",
    "        temperature_tree=temperature_tree,\n",
    "        temperature_therapist=temperature_conversation_Therapist,\n",
    "        temperature_patient=temperature_conversation_Patient,\n",
    "        temperature_eval=temperature_eval,\n",
    "        questionnaire_list=questionnaire_list,\n",
    "        questionnaire_scalars=questionnaire_scalars,\n",
    "        max_token_per_question=max_token_per_question,\n",
    "        partial_conv=partial_conv,\n",
    "        look_ahead=look_ahead\n",
    "    )\n",
    "\n",
    "    return preference_data, messages_Patient_assist, messages_Therapist_assist, conversation, session_endded_by, session_endded_explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate permutations of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# get all permutations\n",
    "permutations = generate_all_permutations(only_expert_therapist=True)\n",
    "print(f\"Number of permutations: {len(permutations)}\")\n",
    "print(f\"perumtation keys: {permutations[0].keys()}\")\n",
    "# permutations is a list of dictionaries, where each dictionary has the following keys: 'counselor_init_utterance', 'counselor_system_prompt', 'patient_system_prompt', 'args'.\n",
    "permutation_pd = pd.DataFrame(permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therapist Level for system prompts\n",
    "good_level = CounselorPersonality.PersonalityLevel.Good\n",
    "mediocre_level = CounselorPersonality.PersonalityLevel.Mediocre\n",
    "bad_level = CounselorPersonality.PersonalityLevel.Bad\n",
    "basic_level = CounselorPersonality.PersonalityLevel.BASIC\n",
    "therapist = CounselorPersonality.choose_random_therapist_name()\n",
    "print(f\"Therapist: {therapist} \\n\")\n",
    "print(\"-\" * 80)\n",
    "# Therapist system prompt (good and bad)\n",
    "therapist_good_system_prompt = CounselorPersonality.build_system_prompt(personality_level=good_level, name=therapist['name'])\n",
    "therapist_mediocre_system_prompt = CounselorPersonality.build_system_prompt(personality_level=mediocre_level, name=therapist['name'])\n",
    "therapist_bad_system_prompt = CounselorPersonality.build_system_prompt(personality_level=bad_level, name=therapist['name'])\n",
    "therapist_basic_system_prompt = CounselorPersonality.build_system_prompt(personality_level=basic_level, name=therapist['name'])\n",
    "\n",
    "# Therapist's initial utterance (good and bad), (for now using the good level)\n",
    "therapist_good_init_utterance = CounselorPersonality.get_init_utterance(personality_level=good_level, name=therapist['name'])\n",
    "therapist_mediocre_init_utterance = CounselorPersonality.get_init_utterance(personality_level=mediocre_level, name=therapist['name'])\n",
    "therapist_bad_init_utterance = CounselorPersonality.get_init_utterance(personality_level=bad_level, name=therapist['name'])\n",
    "therapist_basic_init_utterance = CounselorPersonality.get_init_utterance(personality_level=basic_level, name=therapist['name'])\n",
    "\n",
    "print(f\"Therapist good system prompt: \\n{therapist_good_system_prompt}\\n\")\n",
    "print(f\"Therapist mediocre system prompt: \\n{therapist_mediocre_system_prompt}\\n\")\n",
    "print(f\"Therapist bad system prompt: \\n{therapist_bad_system_prompt}\\n\")\n",
    "print(f\"Therapist basic system prompt: \\n{therapist_basic_system_prompt}\\n\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Therapist good init utterance: \\n{therapist_good_init_utterance}\\n\")\n",
    "print(f\"Therapist mediocre init utterance: \\n{therapist_mediocre_init_utterance}\\n\")\n",
    "print(f\"Therapist bad init utterance: \\n{therapist_bad_init_utterance}\\n\")\n",
    "print(f\"Therapist basic init utterance: \\n{therapist_basic_init_utterance}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preference-Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_id = \"gpt-3.5-turbo\" # Patient model and Eval model\n",
    "max_tokens_per_response = 200 # 100, 200\n",
    "num_init_utterances = 1 # 3, 1\n",
    "num_tree_utterances = 41 # 41, 47\n",
    "temperature_tree = 1.4 # 1.2, 1.4\n",
    "temperature_conversation_Therapist = 0.9 # 0.7, 0.9\n",
    "temperature_conversation_Patient = 0.7\n",
    "\n",
    "# Parameters for usingEval pref_tree\n",
    "questionnaire_list = [13, 14]\n",
    "questionnaire_scalars = [0.5, 0.5]\n",
    "max_token_per_question = 100\n",
    "temperature_eval = 0.2 # 0.1, 0.2\n",
    "# lookAhead = 5 # 0, 3, 5\n",
    "partial_conv_eval = False\n",
    "\n",
    "path_to_save = f\"LLM_DATA/Conversation_Trees/LookAhead_{lookAhead}/TTree1.4_TT0.9_TP0.7_TE0.2_V3\"\n",
    "path_to_save = f\"/content/drive/MyDrive/{path_to_save}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test synthesize_conversation_tree_for_permutation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(permutations)):\n",
    "    print(f\"Permutation {i}\")\n",
    "    print(\"=\" * 160)\n",
    "\n",
    "    # if saved file exists, skip\n",
    "    if os.path.exists(f\"{path_to_save}/pref_data_{i}.csv\"):\n",
    "        print(f\"File exists: {path_to_save}/pref_data_{i}.csv\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    permutation = permutations[i]\n",
    "    preference_data, messages_Patient_assist, messages_Therapist_assist, conversation, session_endded_by, session_endded_explanation = synthesize_conversation_tree_lookAhead_for_permutation(\n",
    "        system_prompt_therapist=therapist_basic_system_prompt, # system prompt for the therapis\n",
    "        system_prompt_patient=permutation[\"patient_system_prompt\"], # system prompt for the patient\n",
    "        therapist_init_utterance=therapist_basic_init_utterance, # therapist's initial utterance\n",
    "        patient_init_utterance=\"\", # patient's initial utterance\n",
    "        therapist_model=base_model, # therapist model\n",
    "        therapis_tokenizer=tokenizer, # therapist tokenizer\n",
    "        client=client, # OpenAI client\n",
    "        model_id=model_id, # model id to use\n",
    "        max_tokens=max_tokens_per_response, # maximum tokens for each response\n",
    "        num_init_utterances=num_init_utterances, # number of turns in the initial conversation (Not including the therapist's initial utterance)\n",
    "        num_tree_utterances=num_tree_utterances, # number of turns in the conversation tree (Not including the therapist's and patient's initial utterances)\n",
    "        temperature_tree=temperature_tree, # temperature for sampling\n",
    "        temperature_conversation_Therapist=temperature_conversation_Therapist, # temperature for the therapist's responses\n",
    "        temperature_conversation_Patient=temperature_conversation_Patient, # temperature for the patient's responses\n",
    "        temperature_eval=temperature_eval, # temperature for evaluation\n",
    "        questionnaire_list=questionnaire_list, # questionnaire IDs\n",
    "        questionnaire_scalars=questionnaire_scalars, # questionnaire scalars\n",
    "        max_token_per_question=max_token_per_question, # maximum tokens for each question\n",
    "        partial_conv=partial_conv_eval, # use partial or full evaluation prompt\n",
    "        look_ahead=lookAhead\n",
    "    )\n",
    "\n",
    "    if preference_data is None:\n",
    "        print(\"Could not generate the required number of valid responses. Skipping permutation number: \", i)\n",
    "        continue\n",
    "\n",
    "    pd_pref_data = pd.DataFrame(preference_data)\n",
    "    print(pd_pref_data.info())\n",
    "    print_conversation(conversation, max_width=80)\n",
    "    print(\"Conversation Length: \", len(conversation))\n",
    "    print(pd_pref_data.tail())\n",
    "    print(\"=\" * 160 + \"\\n\")\n",
    "    \n",
    "    # save check kpoint every permutation\n",
    "    pd_pref_data.to_csv(f\"{path_to_save}/pref_data_{i}.csv\", index=False)\n",
    "\n",
    "\n",
    "pd_pref_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model using our Oracle/Judge\n",
    "- for each permutation of patients synthesize a conversation and evaluate it using the Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = generate_all_permutations(only_expert_therapist=True)\n",
    "print(f\"Number of permutations: {len(permutations)}\")\n",
    "print(f\"perumtation keys: {permutations[0].keys()}\")\n",
    "# permutations is a list of dictionaries, where each dictionary has the following keys: 'counselor_init_utterance', 'counselor_system_prompt', 'patient_system_prompt', 'args'.\n",
    "permutation_pd = pd.DataFrame(permutations)\n",
    "\n",
    "# Hyperparameters for the conversation\n",
    "model_id = \"gpt-3.5-turbo\"\n",
    "max_tokens_per_response = 200 # 100, 200\n",
    "num_uttrances = 49 # not including the therapist's initial utterance (so 50 turns in total)\n",
    "temperature_theapist = 0.9 # 0.7, 1.0, 0.9\n",
    "temperature_patient = 0.7\n",
    "temperature_eval = 0.2\n",
    "questionnaire_list = [13, 14]\n",
    "questionnaire_scalars = [0.5, 0.5]\n",
    "max_token_per_question = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Base #####\n",
    "# path_to_save = \"LLM_DATA/Conversation_with_Eval/Base/Basic_50_TT0.9_TP0.7_TE0.2\"\n",
    "\n",
    "\n",
    "##### Look-Ahead #####\n",
    "path_to_save = f\"LLM_DATA/Conversation_with_Eval/LookAhead_{lookAhead}/TTree1.4_TT0.9_TP0.7_TE0.2_V2\"\n",
    "\n",
    "##################### \n",
    "path_to_save = f\"/content/drive/MyDrive/{path_to_save}\" # save to google drive\n",
    "##################### \n",
    "\n",
    "for i in range(0, len(permutations)):\n",
    "# iterate over all permutations\n",
    "    print(f\"Permutation {i}\")\n",
    "\n",
    "    if os.path.exists(f\"{path_to_save}/conversation_{i}.csv\"):\n",
    "        print(f\"File exists: {path_to_save}/conversation_{i}.csv\")\n",
    "        continue\n",
    "\n",
    "    permutation = permutations[i]\n",
    "    system_prompt_patient = permutation[\"patient_system_prompt\"]\n",
    "    print(\"=\" * 160 + \"\\n\")\n",
    "\n",
    "    # create a conversation\n",
    "    conversation, messages_Patient_assist, messages_Therapist_assist, session_endded_by, session_endded_explanation = synthesize_conversation_therapistModel_patientOpenAI(\n",
    "        system_prompt_therapist=therapist_basic_system_prompt, # system prompt for the therapist\n",
    "        system_prompt_patient=system_prompt_patient, # system prompt for the patient\n",
    "        therapist_init_utterance=therapist_basic_init_utterance, # therapist's initial utterance\n",
    "        patient_init_utterance=\"\", # patient's initial utterance\n",
    "        therapist_model=base_model, # therapist model\n",
    "        therapis_tokenizer=tokenizer, # therapist tokenizer\n",
    "        client=client, # OpenAI client\n",
    "        model_id=model_id, # model id to use\n",
    "        max_tokens=max_tokens_per_response, # maximum tokens for each response\n",
    "        num_utterances=num_uttrances, # number of turns in the conversation (Not including the therapist's initial utterance)\n",
    "        temperature_therapist=temperature_theapist, # temperature for the therapist's responses\n",
    "        temperature_patient=temperature_patient # temperature for the patient's responses\n",
    "    )\n",
    "\n",
    "    if conversation is None:\n",
    "        print(\"Could not generate the required number of valid responses. Skipping permutation number: \", i)\n",
    "        continue\n",
    "\n",
    "    print(\"Conversation Length: \", len(conversation))\n",
    "    # save the conversation to a csv file\n",
    "    pd_conversation = pd.DataFrame(\n",
    "        {\n",
    "            \"conversation\": conversation,\n",
    "            \"session_endded_by\": session_endded_by,\n",
    "            \"session_endded_explanation\": session_endded_explanation\n",
    "        }\n",
    "    )\n",
    "    pd_conversation.to_csv(f\"{path_to_save}/conversation_{i}.csv\", index=False)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "    # Evaluate conversation using the evaluation questionnaire\n",
    "    print(\"Evaluating the conversation...\")\n",
    "    final_score, scores_avg_list, scores_list, results_list = eval_conversation_with_questionnaires(\n",
    "        conversation,\n",
    "        client,\n",
    "        questionnaire_list,\n",
    "        model_id,\n",
    "        temperature_eval,\n",
    "        max_token_per_question,\n",
    "        partial_conv=False,\n",
    "        look_ahead=0,\n",
    "        questionnaire_scalars=questionnaire_scalars\n",
    "    )\n",
    "\n",
    "    if final_score is None:\n",
    "        print(\"Could not generate the required number of valid scores for the conversation. Skipping permutation number: \", i)\n",
    "        # delete the conversation file\n",
    "        os.remove(f\"{path_to_save}/conversation_{i}.csv\")\n",
    "        continue\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(\"-\" * 80)\n",
    "    print(\"results1: \\n\", results_list[0])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"results2: \\n\", results_list[1])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Scores 1:\", scores_list[0])\n",
    "    print(\"Scores 2:\", scores_list[1])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Scores 1 Average:\", scores_avg_list[0])\n",
    "    print(\"Scores 2 Average:\", scores_avg_list[1])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Scores Average:\", (final_score))\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # save the scores\n",
    "    scores_dict = [{\n",
    "        \"scores1\": scores_list[0],\n",
    "        \"scores2\": scores_list[1],\n",
    "        \"scores1_avg\": scores_avg_list[0],\n",
    "        \"scores2_avg\": scores_avg_list[1],\n",
    "        \"scores_avg\": final_score,\n",
    "        \"results1\": results_list[0],\n",
    "        \"results2\": results_list[1]\n",
    "    }]\n",
    "    print(scores_dict)\n",
    "    pd_scores = pd.DataFrame(scores_dict)\n",
    "    # save the scores\n",
    "    pd_scores.to_csv(f\"{path_to_save}/scores_{i}.csv\", index=False)\n",
    "    print(\"=\" * 160 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # go over the saved files and get the scores, delete the conversation and scores files for scores < 2\n",
    "# path_to_save = f\"LLM_DATA/Conversation_with_Eval/LookAhead_{lookAhead}/TTree1.4_TT0.9_TP0.7_TE0.2_V2\"\n",
    "# path_to_save = f\"/content/drive/MyDrive/{path_to_save}\"\n",
    "\n",
    "# for i in range(0, len(permutations)):\n",
    "#     print(f\"Permutation {i}\")\n",
    "#     conversation_file = f\"{path_to_save}/conversation_{i}.csv\"\n",
    "#     scores_file = f\"{path_to_save}/scores_{i}.csv\"\n",
    "\n",
    "#     if not os.path.exists(conversation_file) or not os.path.exists(scores_file):\n",
    "#         print(f\"Files do not exist: {conversation_file}, {scores_file}\")\n",
    "#         continue\n",
    "\n",
    "#     pd_conversation = pd.read_csv(conversation_file)\n",
    "#     pd_scores = pd.read_csv(scores_file)\n",
    "\n",
    "#     # get the scores\n",
    "#     scores_avg = pd_scores[\"scores_avg\"].values[0]\n",
    "#     print(\"Scores Average: \", scores_avg)\n",
    "\n",
    "#     if scores_avg < 2:\n",
    "#         print(\"Scores Average is less than 2. Deleting the conversation and scores files.\")\n",
    "#         os.remove(conversation_file)\n",
    "#         os.remove(scores_file)\n",
    "#         print(\"Deleted the files: \", conversation_file, scores_file)\n",
    "#     print(\"=\" * 160 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

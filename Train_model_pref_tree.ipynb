{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U transformers trl peft bitsandbytes numpy==1.26.4 pandas==2.2.2 torch==2.4.0 datasets wandb\n",
    "# %pip install -q -U transformers==4.44.0 trl==0.9.6 peft==0.12.0 bitsandbytes numpy==1.26.4 pandas==2.2.2 torch==2.4.0 datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qqq flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"cuda version:\", torch.version.cuda)\n",
    "\n",
    "# Define SEED for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper and device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "    print(\"Using flash_attention_2\")\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "    print(\"Using eager\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model):\n",
    "    total_params = 0\n",
    "    trainable_params  = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || total params: {total_params} || trainable%: {100 * trainable_params / total_params}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the therapist model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# log in to the Hugging Face hub (required for private datasets/models)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 7b llama-2 model\n",
    "therapist_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(therapist_model_id, trust_remote_code=True, device_map=device)\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "print(\"Spacial tokens: \", tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LORA config and quantization config \n",
    "lora_config = LoraConfig(\n",
    "    r=16, # 16, 256\n",
    "    lora_alpha=16, # 16, 128\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Define Quantization (Bits and Bytes) config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model, quantized\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    therapist_model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LookAhead and add Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookAhead = 0\n",
    "version = 1\n",
    "num_to_place = {0: \"base\", 1: \"first\", 2: \"second\", 3: \"third\", 4: \"fourth\", 5: \"fifth\", 6: \"sixth\"}\n",
    "\n",
    "########################################### New Adapters ############################################\n",
    "therapist_first_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V1\"\n",
    "therapist_second_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V2\"\n",
    "therapist_third_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V3\"\n",
    "therapist_fourth_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V4\"\n",
    "therapist_fifth_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V5\"\n",
    "therapist_sixth_adapter_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V6\"\n",
    "########################################### New Adapters ############################################\n",
    "\n",
    "adapter_ids = [therapist_first_adapter_id, therapist_second_adapter_id, therapist_third_adapter_id, \n",
    "               therapist_fourth_adapter_id, therapist_fifth_adapter_id, therapist_sixth_adapter_id]\n",
    "\n",
    "display(adapter_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the adapters to the model\n",
    "for i in range(0, version):\n",
    "    print(\"Loading model with adapter \", i+1)\n",
    "    adapter_id = adapter_ids[i]\n",
    "    base_model = PeftModel.from_pretrained(base_model, adapter_id)\n",
    "    base_model = base_model.merge_and_unload()\n",
    "    print(f\"Model loaded with {num_to_place[i]} adapter\")\n",
    "    print(\"Adapter ID: \", adapter_id)\n",
    "\n",
    "print(f\"Model loaded with {version} adapters and {lookAhead} lookAhead\")\n",
    "\n",
    "# Prepare model for KBIT training\n",
    "base_model = prepare_model_for_kbit_training(base_model) # Prepare model for KBIT training\n",
    "print(\"Model prepared for KBIT training\")\n",
    "print_trainable_params(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods to Load the data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to Convert string representations of lists to actual lists\n",
    "def convert_string_to_list(df):\n",
    "    df[\"messages\"] = df[\"messages\"].apply(ast.literal_eval)\n",
    "    df[\"conversation\"] = df[\"conversation\"].apply(ast.literal_eval)\n",
    "    df[\"winning_scores_list\"] = df[\"winning_scores_list\"].apply(ast.literal_eval)\n",
    "    df[\"losing_scores_list\"] = df[\"losing_scores_list\"].apply(ast.literal_eval)\n",
    "    df[\"winning_scores_avg_list\"] = df[\"winning_scores_avg_list\"].apply(ast.literal_eval)\n",
    "    df[\"losing_scores_avg_list\"] = df[\"losing_scores_avg_list\"].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "# Method to load the preference trees\n",
    "def load_preference_trees(data_path, start_index=0, end_index=96):\n",
    "    preference_trees_list = []\n",
    "    for i in range(start_index, end_index):\n",
    "        with open(os.path.join(data_path, f\"pref_data_{i}.csv\"), \"r\") as f:\n",
    "            pref_tree = pd.read_csv(f)\n",
    "            pref_tree[\"tree_index\"] = i\n",
    "            preference_trees_list.append(pref_tree)\n",
    "    # Concatenate all the preference trees into a single dataframe\n",
    "    preference_trees_df = pd.concat(preference_trees_list, ignore_index=True)\n",
    "    num_of_rows_before = preference_trees_df.shape[0]\n",
    "    #get only rows with NaN values\n",
    "    preference_trees_NaN = preference_trees_df[preference_trees_df.isna().any(axis=1)]\n",
    "    # Drop rows with missing values\n",
    "    preference_trees_df = preference_trees_df.dropna()\n",
    "    num_of_rows_after = preference_trees_df.shape[0]\n",
    "    num_rows_removed = num_of_rows_before - num_of_rows_after\n",
    "    # Convert string representations of lists to actual lists\n",
    "    preference_trees_df = convert_string_to_list(preference_trees_df)\n",
    "    return preference_trees_df, preference_trees_list, num_rows_removed, preference_trees_NaN\n",
    "\n",
    "# Method to add prompts column to the dataframe (aplly chat tamplet to the messages)\n",
    "def add_prompts_column(df, tokenizer):\n",
    "    prompt_list = [tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False) for message in df[\"messages\"]]\n",
    "    df[\"prompt\"] = prompt_list\n",
    "    return df\n",
    "\n",
    "# Method to add prompt length column to the dataframe\n",
    "def add_prompt_length_column(df, tokenizer):\n",
    "    prompt_length_list = [len(tokenizer.encode(prompt)) for prompt in df[\"prompt\"]]\n",
    "    df[\"prompt_length\"] = prompt_length_list\n",
    "    return df\n",
    "\n",
    "# Method to add conversation length (number of turns) column to the dataframe\n",
    "def add_conversation_length_column(df):\n",
    "    conversation_length_list = [len(conversation) for conversation in df[\"conversation\"]]\n",
    "    df[\"conversation_length\"] = conversation_length_list\n",
    "    return df\n",
    "\n",
    "# Method to add eos token to the end of the responses\n",
    "def add_end_token_to_responses(df, tokenizer):\n",
    "    df[\"winning_response\"] = [response + tokenizer.eos_token for response in df[\"winning_response\"]]\n",
    "    df[\"losing_response\"] = [response + tokenizer.eos_token for response in df[\"losing_response\"]]\n",
    "    return df\n",
    "\n",
    "# Method to remove rows where the winning response is the same as the losing response\n",
    "def remove_duplicate_responses(df):\n",
    "    df_without_duplicates = df[df[\"winning_response\"] != df[\"losing_response\"]]\n",
    "    df_with_duplicates = df[df[\"winning_response\"] == df[\"losing_response\"]]\n",
    "    return df_without_duplicates, df_with_duplicates\n",
    "\n",
    "# Method to create the dpo_prefernce_data (Dict with keys: \"prompt\", \"chosen\", \"rejected\")\n",
    "def create_preference_data(preference_trees_df, score_threshold=0.2,\n",
    "                           prompt_column_name=\"prompt\", winning_response_column_name=\"winning_response\", losing_response_column_name=\"losing_response\", \n",
    "                           winning_score_column_name=\"winning_score_final\", losing_score_column_name=\"losing_score_final\", min_score=0.0, max_score=5.0):\n",
    "    # update the number of rows before dropping rows not within the min_score and max_score\n",
    "    num_of_rows_before = preference_trees_df.shape[0]\n",
    "    # drop rows where the winning score or losing score is less than min_score or greater than max_score\n",
    "    preference_trees_df = preference_trees_df[(preference_trees_df[winning_score_column_name] >= min_score) & (preference_trees_df[winning_score_column_name] <= max_score)]\n",
    "    preference_trees_df = preference_trees_df[(preference_trees_df[losing_score_column_name] >= min_score) & (preference_trees_df[losing_score_column_name] <= max_score)]\n",
    "    # print the number of rows before and after dropping rows not within the min_score and max_score\n",
    "    print(f\"Number of rows in total: {num_of_rows_before}\")\n",
    "    print(f\"Number of rows removed due to min_score or max_score: {num_of_rows_before - preference_trees_df.shape[0]}\")\n",
    "    print(f\"Number of rows remaining: {preference_trees_df.shape[0]}\")\n",
    "\n",
    "    # update the number of rows before thresholding\n",
    "    num_of_rows_before = preference_trees_df.shape[0] \n",
    "    # drop rows where the winning score < losing score + threshold\n",
    "    preference_data = preference_trees_df[preference_trees_df[winning_score_column_name] >= preference_trees_df[losing_score_column_name] + score_threshold]\n",
    "    num_of_rows_after = preference_data.shape[0]\n",
    "    # print the number of rows before and after thresholding\n",
    "    print(f\"Number of rows in total: {num_of_rows_before}\")\n",
    "    print(f\"Number of rows removed due to threshold: {num_of_rows_before - num_of_rows_after}\")\n",
    "    print(f\"Number of rows remaining: {num_of_rows_after}\")\n",
    "\n",
    "    # Create the dpo_dataset_dict (Preference Data) (Dict with keys: \"prompt\", \"chosen\", \"rejected\")\n",
    "    dpo_dataset_dict = {\n",
    "    \"prompt\": preference_data[prompt_column_name].tolist(),\n",
    "    \"chosen\": preference_data[winning_response_column_name].tolist(),\n",
    "    \"rejected\": preference_data[losing_response_column_name].tolist(),\n",
    "    }\n",
    "    # Create the dpo_dataset from the dpo_dataset_dict\n",
    "    dpo_dataset = Dataset.from_dict(dpo_dataset_dict)\n",
    "    \n",
    "    return dpo_dataset, preference_data\n",
    "\n",
    "# Method to print the conversation with word wrapping\n",
    "def print_conversation(conversation, max_width=80):\n",
    "    \"\"\"\n",
    "    Print the conversation with word wrapping.\n",
    "\n",
    "    Parameters:\n",
    "        - conversation: A list of strings representing the conversation. (Therapist and Patient messages alternately, starting with the Therapist)\n",
    "        - max_width: The maximum width for word wrapping. Default is 80.\n",
    "    \"\"\"\n",
    "    for i, message in enumerate(conversation):\n",
    "        role = \"[THERAPIST]\" if i % 2 == 0 else \"[PATIENT]\"\n",
    "        print(f\"{role}: \\n{textwrap.fill(message, width=max_width)} \\n\")\n",
    "\n",
    "# Method to get only the final conversations for each tree index\n",
    "def get_df_for_final_conversations_for_each_tree_index(df):\n",
    "    final_conversations_list = []\n",
    "    for tree_index in df[\"tree_index\"].unique():\n",
    "        final_conversations = df[(df[\"tree_index\"] == tree_index) & (df[\"conversation_length\"] == df[df[\"tree_index\"] == tree_index][\"conversation_length\"].max())]\n",
    "        final_conversations_list.append(final_conversations)\n",
    "    final_conversations_df = pd.concat(final_conversations_list, ignore_index=True)\n",
    "    return final_conversations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and preprocess it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"LLM_DATA/Conversation_Trees/LookAhead3_FullEval_TTree1.4_TLoop0.7_TEval0.2_V4.0\"\n",
    "# data_path = f\"LLM_DATA/Conversation_Trees/LookAhead_{lookAhead}/TTree1.4_TT0.9_TP0.7_TE0.2_V2\"\n",
    "\n",
    "data_path = f\"LLM_DATA/Conversation_Trees/LookAhead_{lookAhead}/TTree1.4_TT0.9_TP0.7_TE0.2_V{version+1}\"\n",
    "data_path = f\"/content/drive/MyDrive/{data_path}\"\n",
    "\n",
    "# Load the preference trees\n",
    "preference_trees_df, preference_trees_list, num_rows_removed, preference_trees_NaN = load_preference_trees(data_path, start_index=0, end_index=96)\n",
    "print(f\"Number of rows removed due to missing values: {num_rows_removed}\")\n",
    "preference_trees_df = add_prompts_column(preference_trees_df, tokenizer)\n",
    "preference_trees_df = add_prompt_length_column(preference_trees_df, tokenizer)\n",
    "preference_trees_df = add_conversation_length_column(preference_trees_df)\n",
    "# Remove rows where the winning response is the same as the losing response\n",
    "preference_trees_df, duplicate_responses_df = remove_duplicate_responses(preference_trees_df)\n",
    "display(preference_trees_df.columns)\n",
    "display(preference_trees_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preference data for the model\n",
    "dpo_dataset, preference_trees_filtered_df = create_preference_data(preference_trees_df, score_threshold=0.1)\n",
    "# shuffle the dataset\n",
    "dpo_dataset = dpo_dataset.shuffle(seed=42)\n",
    "# split the dataset into training and validation sets\n",
    "dpo_dataset = dpo_dataset.train_test_split(test_size=0.01)\n",
    "print(\"dpo_dataset length: \", len(dpo_dataset))\n",
    "display(dpo_dataset[\"train\"][0])\n",
    "display(dpo_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset[\"train\"].shape, dpo_dataset[\"test\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training algorithms\n",
    "- DPO\n",
    "- ORPO\n",
    "- Soon: PPO, KTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO_model_id = \"LBK95/Llama-2-7b-hf-DPO-LookAhead3_FullEval_TTree1.4_TLoop0.7_TEval0.2_Filter0.2_V4.0\"\n",
    "DPO_model_id = f\"LBK95/Llama-2-7b-hf-DPO-LookAhead-{lookAhead}_TTree1.4_TT0.9_TP0.7_TE0.2_V{version+1}\"\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\", # other options: \"linear\", \"cosine\", \"cosine_with_restarts\"\n",
    "    #max_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=0.25,\n",
    "    logging_steps=1,\n",
    "    eval_steps=0.1,\n",
    "    output_dir=DPO_model_id,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=10,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    base_model,\n",
    "    None,\n",
    "    args=training_args,\n",
    "    train_dataset=dpo_dataset[\"train\"],\n",
    "    eval_dataset=dpo_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=2048,\n",
    "    max_length=2176,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train()\n",
    "\n",
    "# push the trained model and tokenizer to the hub\n",
    "dpo_trainer.push_to_hub()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO_model_id = \"LBK95/Llama-2-7b-hf-eval_threapist-ORPO-version-1\"\n",
    "\n",
    "# # Training arguments\n",
    "# orpo_args = ORPOConfig(\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     gradient_checkpointing=True,\n",
    "#     learning_rate=8e-6,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     #max_steps=1000,\n",
    "#     num_train_epochs=1,\n",
    "#     save_strategy=\"steps\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_steps=0.25,\n",
    "#     logging_steps=1,\n",
    "#     eval_steps=0.2,\n",
    "#     output_dir=ORPO_model_id,\n",
    "#     warmup_steps=10,\n",
    "#     bf16=True,\n",
    "#     report_to=\"wandb\",\n",
    "#     push_to_hub=True,\n",
    "#     remove_unused_columns=False,\n",
    "#     lr_scheduler_type=\"linear\",\n",
    "#     max_length=2048,\n",
    "#     max_prompt_length=1024,\n",
    "#     beta=0.1,\n",
    "# )\n",
    "\n",
    "# # Create DPO trainer\n",
    "# orpo_trainer = ORPOTrainer(\n",
    "#     model=base_model,\n",
    "#     args=orpo_args,\n",
    "#     train_dataset=dpo_dataset[\"train\"],\n",
    "#     eval_dataset=dpo_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     peft_config=lora_config,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orpo_trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orpo_trainer.train()\n",
    "\n",
    "# # push the trained model and tokenizer to the hub\n",
    "# orpo_trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
